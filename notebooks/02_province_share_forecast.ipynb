{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f248c75",
   "metadata": {},
   "source": [
    "# Province Share Modeling (Synthetic Demo)\n",
    "\n",
    "This notebook is cleaned for GitHub and uses **synthetic sample data**.\n",
    "\n",
    "## How to run\n",
    "1) `pip install -r requirements.txt`\n",
    "2) Run all cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"../examples/sample_province_share.csv\"\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[\"date\"]) \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c23c2e",
   "metadata": {},
   "source": [
    "## Project: States Analysis v4\n",
    "\n",
    "# final version. run THIS\n",
    "\n",
    " This Notebook was maintained to predict each states' portion of total order Order for each day\n",
    "\n",
    "\n",
    "<sub>Customer Insights @ Digikala</sub>\n",
    "\n",
    "<sub>10 June 2024</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import optuna\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5123bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = r'/content/dataaa_result_danesh_ready_for_add_oct13.xlsx'\n",
    "\n",
    "#sheet = 'Danesh p'\n",
    "#sheet = 'Shadabad p'\n",
    "\n",
    "data = pd.read_excel(data_file)\n",
    "\n",
    "data.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73473d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Unnamed: 0_y','Unnamed: 0_x'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3711ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f10cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[data['PersianDate']>=14010101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempp=data[data['oct'].isnull()]\n",
    "temppp=tempp[tempp['PersianDate']<14030101 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b1dc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "temppp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ed8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temppp['state_name_en'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0d737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in temppp['state_name_en'].unique():\n",
    "    bin = data[data['state_name_en'] == city].copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    bin['oct'].fillna(bin['oct'].mean(), inplace=True)  # Fill NaN values\n",
    "    data.loc[data['state_name_en'] == city, 'oct'] = bin['oct']  # Update the main DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3612bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['PersianDate']<=14030930].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0ecd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['trend','pct_change','dollar','plus_sub_sum_past_30days','plus_subscription'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ea1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prov=data['State'].unique()\n",
    "def objective(trial):\n",
    "\n",
    "  reg = ExtraTreesRegressor(max_depth=trial.suggest_int(\"max_depth\", 50, 100))\n",
    "  reg.fit(X_train, y_train)\n",
    "  y_pred = reg.predict(X_test)\n",
    "  return mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfbf5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=data.iloc[:(-2*31*30)+31*30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[-2*31*30+31*29:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f980022",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame()\n",
    "#train_data = data.iloc[365*31:-4*31*30]\n",
    "for p in prov:\n",
    "    print(f'prov is  :  {p}')\n",
    "    globals()[p] = train_data[train_data['State'] == p]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        globals()[p].drop(columns=['portion', 'state_name_en', 'State','order_count']),\n",
    "        globals()[p]['portion'], test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f'thiiiiiiiiiiiiiiiiiiiiiiiis {type(X_train)}')\n",
    "\n",
    "    # Setup Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=15)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    et = ExtraTreesRegressor(**best_params)\n",
    "    et.fit(X_train, y_train)\n",
    "    y_pred = et.predict(X_test)\n",
    "\n",
    "    print(f'cat is  :  {p}')\n",
    "    plt.plot(y_test.values[0:100], \"b\")\n",
    "    plt.plot(y_pred[0:100], \"r--\")\n",
    "    plt.show()\n",
    "\n",
    "    # Process test data\n",
    "    test_data =data.iloc[-2*31*30+31*29:]\n",
    "    globals()[p] = test_data[test_data['State'] == p]\n",
    "\n",
    "    # Check if there is any data for this state\n",
    "    if globals()[p].shape[0] == 0:\n",
    "        print(f\"No data available for state {p} in the test set.\")\n",
    "    else:\n",
    "        print(f'prov is under test  :  {p}')\n",
    "        globals()[p].info()\n",
    "\n",
    "\n",
    "        output['date'] = globals()[p]['PersianDate']\n",
    "        #output['y_actual'] = globals()[p]['portion']\n",
    "\n",
    "        # Drop columns without using 'inplace=True' and check if data exists\n",
    "        X_test = globals()[p].drop(columns=['portion', 'state_name_en', 'State','order_count'])\n",
    "        if X_test.shape[0] == 0:\n",
    "            print(f\"No valid data for prediction in state {p}\")\n",
    "        else:\n",
    "            output[p] = et.predict(X_test)\n",
    "output['date'] = globals()[p]['PersianDate']\n",
    "output.to_excel(f'result25+1.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b32f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200f9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "index = 62620\n",
    "#index = 3954\n",
    "train_data ,test_data = trainval_test_split_by_index(data, index)\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame(test_data['PersianDate'].unique(), columns=['PersianDate'])\n",
    "\n",
    "i=0\n",
    "for state in states:\n",
    "  i += 1\n",
    "  print(f\" {i} / {len(states)}. {state} ... \", end='' )\n",
    "\n",
    "  save = {}\n",
    "  # State Segment Selection\n",
    "  train = state_selection(train_data, state)\n",
    "  train.fillna(train.mean())\n",
    "\n",
    "  ## X and y preparation\n",
    "  X_train1, X_test1, y_train1, y_test1 = serial_train_test_split(train, test_size=0.05)\n",
    "  #X_train1, X_test1, y_train1, y_test1 = serial_train_test_split(train, test_size=0.0001)###################\n",
    "\n",
    "\n",
    "  ## Scaling\n",
    "  #X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler, scaler_y = scale(X_train1, X_test1, y_train1, y_test1)\n",
    "  X_train_scaled = X_train1\n",
    "  X_test_scaled = X_test1\n",
    "  y_train_scaled = y_train1\n",
    "  y_test_scaled = y_test1\n",
    "\n",
    "\n",
    "  ## Grid Search for Best Parameters\n",
    "  #best_params = grid(X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled)\n",
    "\n",
    "\n",
    "  ## Model Defenition\n",
    "  model = XGBRegressor('''n_estimators=500, max_depth=best_params['max_depth'], learning_rate=0.1,\n",
    "                       reg_lambda=best_params['reg_lambda'], reg_alpha=best_params['reg_alpha'],\n",
    "                       min_child_weight=best_params['min_child_weight'], random_state=42,\n",
    "                       subsample=best_params['subsample']''')\n",
    "\n",
    "\n",
    "  # Fit the model on training data\n",
    "  model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "  # Prediting\n",
    "  y_train_pred = model.predict(X_train_scaled)\n",
    "  y_test_pred = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "  # Calculate RMSE and R2 Score for training and testing sets\n",
    "  train_rmse = rmse(y_train_scaled, y_train_pred)\n",
    "  test_rmse = rmse(y_test_scaled, y_test_pred)\n",
    "  #train_score = model.score(X_train_scaled, y_train_scaled)\n",
    "  # test_score = model.score(X_test_scaled, y_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "  # Testing Phase\n",
    "  test = state_selection(test_data, state)\n",
    "  #X_scaled = scaler.transform(test)\n",
    "  X_scaled = test\n",
    "  predict = model.predict(X_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b8ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # y inversion\n",
    "  #output = scaler_y.inverse_transform(predict.reshape(-1, 1))\n",
    "\n",
    "  #output = outlier_normalization(output, y_train1)\n",
    "\n",
    "  # SHAP EXPLAINER\n",
    "  #explainer = shap.TreeExplainer(model)\n",
    "  #explanation = explainer(X_scaled)\n",
    "\n",
    "  shap_values = explanation.values\n",
    "# make sure the SHAP values add up to marginal predictions\n",
    "  np.abs(shap_values.sum(axis=1) + explanation.base_values - predict).max()\n",
    "  ## Saving Phase\n",
    "  save['model'] = model\n",
    "  save['train_rmse'] = train_rmse\n",
    "  save['test_rmse'] = test_rmse\n",
    "  # save['train_score'] = train_score\n",
    "  # save['test_score'] = test_score\n",
    "  save['output'] = output\n",
    "  save['X_train_scaled'] = X_train_scaled\n",
    "  save['shap_values'] = shap_values\n",
    "  save['explanation'] = explanation\n",
    "  models[state] = save\n",
    "\n",
    "  ## Creating output Dataframe\n",
    "  output_df[state]= output\n",
    "  print(\"DONE\")\n",
    "\n",
    "\n",
    "# output_df.to_excel( 'StatesAnalysis_output.xlsx', sheet_name='predictions')\n",
    "\n",
    "# with open('StatesAnalysiModels.json', 'w') as fp:\n",
    "#     json.dump(models, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c7a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df = output_df.copy()\n",
    "sums = np.array(output_df.drop('PersianDate', axis=1).sum(axis=1)).reshape(-1,1)\n",
    "normalized_df = normalized_df.loc[:, normalized_df.columns != 'PersianDate'] / sums\n",
    "normalized_df = pd.concat([output_df['PersianDate'], normalized_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88743678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write = 'StatesAnalysis_Danesh.xlsx'\n",
    "write = 'StatesAnalysis_Shad.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(write) as writer:\n",
    "    output_df.to_excel(writer, sheet_name='Predictions')\n",
    "    normalized_df.to_excel(writer, sheet_name='Normalized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16886fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for province, configs in models.items():\n",
    "# Create a summary plot with the bar plot type\n",
    "\n",
    "  shap.summary_plot(configs[\"shap_values\"], configs[\"X_train_scaled\"], plot_type='bar', show=False)\n",
    "  # Customize the plot appearance\n",
    "  plt.title(\"SHAP Summary Plot - Feature Importance\")\n",
    "  plt.xlabel(\"SHAP Value\", fontsize=12)\n",
    "  plt.ylabel(\"Feature\", fontsize=12)\n",
    "  plt.xticks(fontsize=10)\n",
    "  plt.yticks(fontsize=10)\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(f'fresh_{province}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e46e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for province, configs in models.items():\n",
    "\n",
    "  shap.plots.beeswarm(explanation)\n",
    "  plt.savefig(f'fresh_{province}_swarm.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
